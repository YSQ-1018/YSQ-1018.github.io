## 梯度下降选取梯度反方向进行优化的原因及其锯齿效应
# 梯度下降算法描述
<font size=3,line-height:3>$$min\,f(x)$$,起始点为$x_{k}$，更新方向为点$x_{k}$处的负(反/逆)梯度方向，更新步长为$\lambda \,(\lambda>0)$，更新公式为$x_{k+1}=x_{k}-\lambda\nabla f(x_{k})$。

## 梯度下降选取梯度反方向优化的原因
有许多文章通过导数、方向导数及梯度的概念及意义对该问题进行了解释，本文使用更加直接的Taylor展式的方式对该问题做出解释:


设待优化求解的问题为$min\,f(x)$,起始点为$x_{k}$，更新方向为$p{k}$ (一个单位方向向量)，更新步长为$\lambda \,(\lambda>0)$，则第$k+1$个点为$x_{k+1}=x_{k}+\lambda p_{k}$，此处设函数$f_(x)$为多元函数，其梯度为一个向量。


我们希望$$f(x_{k+1})$$的值越小越好，现考虑其一阶(不严谨的，省略了余项)Taylor展式:
&emsp;&emsp;&emsp;&emsp;$f(x_{k+1})=f(x_{k}+\lambda p_{k})$
&emsp;&emsp;&emsp;&emsp;$f(x_{k}+\lambda p_{k})=f(x_{k})+\lambda[\nabla f(x_k)]^{T}\,p_{k}+o(.)  \qquad(1)$

----------------------------

$(1)$式使用的Taylor展开不太常见，给出简要的不严谨的说明:

单变量函数的常见的Taylor展式，如$f(x)在x_{1}处展开$

$$f(x) = f(x_{1})+f^{'}{(x_{1})}(x-x_{1})+\frac{f^{''}(x_{1})}{2!}(x-x_{1})^2+...+\frac{f^{(n)}(x_{1})}{n!}(x-x_{1})^n$$
<br/>
现令$x=x+h$，有:
$f(x+h) = f(x_{1})+f^{'}{(x_{1})}(x+h-x_{1})+\frac{f^{''}(x_{1})}{2!}(x+h-x_{1})^2+...+\frac{f^{(n)}(x_{1})}{n!}(x+h-x_{1})^n$
<br/>
由于展开点$x_{1}$的选择是任意的，令$x_{1}=h$有: (此时在$x=h$附近进行展开)
$f(x+h) = f(h)+f^{'}{(h)}(x)+\frac{f^{''}(h)}{2!}(x)^2+...+\frac{f^{(n)}(h)}{n!}(x)^n \qquad(\star)$
<br/>
上式可以扩展至多元函数，将乘法变为向量内积即可，如考虑$(1)$式的单变量形式:
$f(x_{k}+\lambda p_{k})=f(x_{k})+\nabla f(x_k)\,\lambda p_{k}+o(.)$<br/>
对其扩展即可得
$f(x_{k}+\lambda p_{k})=f(x_{k})+\lambda[\nabla f(x_k)]^{T}\,p_{k}+o(.)  \qquad(1)$

<br/>实际上Taylor展示的一阶形式$f(x+h) = f(h)+f^{'}{(h)}x$ 换一种写法即为 (此时在$x=h$附近进行展开，$x\to h$):
$f^{'}(h) ={\lim_{x\to h}} \frac{f(x+h)-f(h)}{x}$，这与一阶导数定义一致。$\\$

---------
回到$(1)$式，希望该式越小，由于步长$\lambda >0$，则应使$[\nabla f(x_k)]^{T}\,p_{k} = <\nabla f(x_k),p_{k}>\,$越小越好，<br/>
$<\nabla f(x_k),p_{k}> = ||\nabla f(x_k)||\,||p_{k}||\,cos<\nabla f(x_k),p_{k}>$，<br/>
显然当$cos<\nabla f(x_k),p_{k}> = -1$时，即$\nabla f(x_k)和p_{k}\,$的夹角为180度时，原函数达到最小值<br/>
即取$p_{k}$为梯度反方向的单位方向向量作为更新方向，即$x_{k+1}=x_{k}-\lambda\nabla f(x_{k})$

至此给出了梯度下降选取梯度反方向优化的原因。

## 梯度下降算法的锯齿效应(zigzag)
梯度下降的锯齿效应描述的是该算法运行是，**当前点的搜索方向**与更新后**下一个点的搜索方向**相正交，仍考虑此问题:
<br/>
$min\,f(x)$,起始点为$x_{k}$，更新方向为点$x_{k}$处的负(反/逆)梯度方向，更新步长为$\lambda \,(\lambda>0)$，更新公式为$x_{k+1}=x_{k}-\lambda\nabla f(x_{k})$。
<br/>
则锯齿效应的数学表示为:$\quad<\nabla f(x_{k+1}),\nabla f(x_{k})> = 0$
<br/>
该式的证明如下:
在使用梯度下降算法优化求解目标函数的最小值时，我们已经确定了搜索的方向为当前点$x_{k}$的反梯度方向，但仍为确定前进的步长。
<br/>
为了更简便的说明问题，此处假设该函数为单变量函数且可微，则最优步长的求解即令目标函数$f(.)$在点$x_{k+1}$关于步长$\lambda$的偏导数为0，即:$\frac{\partial f(x_{k+1})}{\partial \lambda}=\nabla f(x_{k+1}) \frac{\partial \{x_{k}-\lambda\nabla f(x_{k})\}} {\partial \lambda}=-\nabla f(x_{k+1})\nabla f(x_{k})=0$
<br/>
将上述结果推广至多元函数有$\quad<\nabla f(x_{k+1}),\nabla f(x_{k})> = 0$，
这便是梯度下降算法产生锯齿现象的原因。


</font>
